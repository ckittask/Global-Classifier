# LLM Provider configuration
provider:
  name: "ollama"
  model_name: "gemma3:1b-it-qat"
  api_url: "http://ollama:11434"
  timeout: 60
  max_retries: 3
  retry_delay: 5

# API connection settings
api:
  url: "http://localhost:8000"
  timeout_seconds: 30

# Directory paths
directories:
  input: "data"
  output: "output_datasets"
  templates: "templates"
  user_configs: "user_configs"
  
# Default generation settings (can be overridden per generation request)
generation:
  default_num_examples: 5
  default_language: "et"
  parameters:
    temperature: 0.7
    max_tokens: 4096

# Dataset generation configuration
dataset_generation:
  structure_name: "single_question"
  prompt_template_name: "institute_topic_question"
  traversal_strategy: "recursive"
  output_format: "json"
  num_samples: 10
  post_processing: "aggregation"  # Options: "zip", "aggregation"
  # Aggregation-specific configuration (only used when post_processing = "aggregation")
  aggregation:
    output_filename: "12"
    merge_strategy: "combine_arrays"
    include_metadata: true
  parameters:
    language: "et"
    temperature: 0.7
    redundancy_factor: 1.5
    language_name: "Estonian"
    difficulty: "medium"
    style: "clear and concise"
  filter: {}
    
# Processing settings
processing:
  wait_between_requests: 1

# MLflow tracking
mlflow:
  experiment_name: "synthetic_data_generation"

# Data source configuration
data_sources:
  default:
    strategy: "pattern"
    base_path: "data"
    patterns: ["**/*.json", "**/*.txt"]
    recursive: true
    filter:
      extensions: ["json", "txt"]

callback:
  url: "http://ruuter-public:8086/global-classifier/data/callback"
  max_retries: 3
  timeout: 30