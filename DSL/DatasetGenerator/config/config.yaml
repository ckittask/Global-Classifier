# LLM Provider configuration
provider:
  name: "ollama"
  model_name: "gemma3:1b-it-qat"
  api_url: "http://ollama:11434"
  timeout: 60
  max_retries: 3
  retry_delay: 5

# API connection settings
api:
  url: "http://localhost:8000"
  timeout_seconds: 30

# Directory paths
directories:
  input: "data"
  output: "output_datasets"
  templates: "templates"
  user_configs: "user_configs"
  
# Default generation settings (can be overridden per generation request)
generation:
  default_num_examples: 5
  default_language: "et"
  parameters:
    temperature: 0.7
    max_tokens: 4096

# Dataset generation configuration
dataset_generation:
  structure_name: "single_question"
  prompt_template_name: "institute_topic_question"
  traversal_strategy: "recursive"
  output_format: "json"
  num_samples: 5
  post_processing: "aggregation"  # Options: "zip", "aggregation"
  # Aggregation-specific configuration (only used when post_processing = "aggregation")
  aggregation:
    output_filename: "12"
    merge_strategy: "combine_arrays"
    include_metadata: true
    field_mapping:
      enabled: true
      payload_to_output:
        agency_name: agency_name
        agency_id: agency_id
      defaults:
        id: auto_increment
      content_fields:
        - question

# Language and style settings for dataset generation
  parameters:
    language: "et"
    temperature: 0.7
    redundancy_factor: 1.5
    language_name: "Estonian"
    difficulty: "medium"
    style: "clear and concise"
    system_prompt: "You are a helpful assistant for generating synthetic questions for given contexts."
  filter: {}
    
# Processing settings
processing:
  wait_between_requests: 1

# MLflow tracking
mlflow:
  experiment_name: "synthetic_data_generation"

# Data source configuration
data_sources:
  default:
    strategy: "pattern"
    base_path: "data"
    patterns: ["**/*.json", "**/*.txt"]
    recursive: true
    filter:
      extensions: ["json", "txt"]

callback:
  url: "http://ruuter-public:8086/global-classifier/data/callback"
  max_retries: 3
  timeout: 30

# Relevance Score Analysis
relevance_score:
  enabled: true
  embedding_model: "paraphrase-multilingual-mpnet-base-v2"
  segment_weight: 0.6
  query_weight: 0.3
  term_weight: 0.1
  threshold_good: 0.7
  threshold_acceptable: 0.5
  min_df: 1
  max_df: 0.9
  ngram_range: (1, 2)

# Information Coverage Analysis
information_coverage:
  enabled: true
  similarity_threshold: 0.5

# Model settings
models:
  embedding_model: "paraphrase-multilingual-mpnet-base-v2"
  qualitative_model: "google/gemma-2-2b-it"
  use_4bit_quantization: true